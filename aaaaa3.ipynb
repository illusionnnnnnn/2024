{"cells":[{"cell_type":"markdown","metadata":{},"source":["# 👾Qwen2大模型微调入门\n","\n","作者：林泽毅\n","\n","教程文章：https://zhuanlan.zhihu.com/p/702491999  \n","\n","显存要求：10GB左右  \n","\n","实验过程看：https://swanlab.cn/@ZeyiLin/Qwen2-fintune/runs/cfg5f8dzkp6vouxzaxlx6/chart"]},{"cell_type":"markdown","metadata":{},"source":["## 1.安装环境1\n","\n","本案例测试于modelscope==1.14.0、transformers==4.41.2、datasets==2.18.0、peft==0.11.1、accelerate==0.30.1、swanlab==0.3.9"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%pip install torch swanlab modelscope transformers datasets peft pandas accelerate"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":["如果是第一次使用SwanLab，则前往[SwanLab](https://swanlab.cn)注册账号后，在[用户设置](https://swanlab.cn/settings/overview)复制API Key，如果执行下面的代码："]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## 2. 数据集加载\n","\n","1. 在[zh_cls_fudan-news - modelscope](https://modelscope.cn/datasets/huangjintao/zh_cls_fudan-news/files)下载train.jsonl和test.jsonl到同级目录下。\n","\n","<img src=\"../assets/dataset.png\" width=600>"]},{"cell_type":"markdown","metadata":{},"source":["2. 将train.jsonl和test.jsonl进行处理，转换成new_train.jsonl和new_test.jsonl"]},{"cell_type":"code","execution_count":5,"metadata":{"trusted":true},"outputs":[],"source":["import pandas as pd\n","import json\n","import os\n","\n","def dataset_tsv_to_jsonl(origin_path, new_path):\n","    \"\"\"\n","    将 TSV 数据集转换为 JSONL 格式\n","    \"\"\"\n","    df = pd.read_csv(origin_path, sep='\\t')\n","\n","    # 定义要保存的消息结构\n","    messages = []\n","\n","    for index, row in df.iterrows():\n","        message = {\n","            \"instruction\": \"根据以下评论内容，预测评论者的个性特质。\",\n","            \"input\": row['comment'],\n","            \"output\": {\n","                \"personality_conscientiousness\": row['personality_conscientiousness'],\n","                \"personality_openess\": row['personality_openess'],\n","                \"personality_extraversion\": row['personality_extraversion'],\n","                \"personality_agreeableness\": row['personality_agreeableness'],\n","                \"personality_stability\": row['personality_stability']\n","            }\n","        }\n","        messages.append(message)\n","\n","    # 保存重构后的 JSONL 文件\n","    with open(new_path, \"w\", encoding=\"utf-8\") as file:\n","        for message in messages:\n","            file.write(json.dumps(message, ensure_ascii=False) + \"\\n\")\n","\n","# 设置原始 TSV 文件和目标 JSONL 文件的路径\n","train_tsv_path = \"/home/wangyanan/transformer-code1/transformers-code/13-qwen2.5-7b/dataset/train.tsv\"  # 假设 TSV 文件位于同一目录下\n","new_train_path = \"/home/wangyanan/transformer-code1/transformers-code/13-qwen2.5-7b/dataset/new_personality_train.jsonl\"\n","\n","\n","# test_tsv_path = \"/kaggle/input/qwen25/test.tsv\"  # 假设 TSV 文件位于同一目录下\n","# new_test_path = \"/kaggle/input/qwen25/new_personality_test.jsonl\"\n","# 检查目标文件是否存在，如果不存在，则执行转换\n","if not os.path.exists(new_train_path):\n","    dataset_tsv_to_jsonl(train_tsv_path, new_train_path)\n","\n","# if not os.path.exists(new_test_path):\n","#     dataset_tsv_to_jsonl(test_tsv_path, new_test_path)\n","\n","train_df = pd.read_json(new_train_path, lines=True)[:792]  # 取前1000条做训练（可选）\n","# test_df = pd.read_json(new_test_path, lines=True)[:10]  # 取前10条做主观评测\n"]},{"cell_type":"markdown","metadata":{},"source":["## 3. 下载/加载模型和tokenizer"]},{"cell_type":"code","execution_count":3,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-10-14 11:29:00,775 - modelscope - WARNING - Using branch: master as version is unstable, use with caution\n","Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.40s/it]\n"]}],"source":["from modelscope import snapshot_download, AutoTokenizer\n","from transformers import AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n","import torch\n","\n","# 在modelscope上下载Qwen模型到本地目录下\n","model_dir = snapshot_download(\"Qwen/Qwen2.5-7B\", cache_dir=\"./\", revision=\"master\")\n","\n","# Transformers加载模型权重\n","tokenizer = AutoTokenizer.from_pretrained(\"./Qwen/Qwen2___5-7B/\", use_fast=False, trust_remote_code=True)\n","model = AutoModelForCausalLM.from_pretrained(\"./Qwen/Qwen2___5-7B/\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n","model.enable_input_require_grads()  # 开启梯度检查点时，要执行该方法"]},{"cell_type":"markdown","metadata":{},"source":["## 4. 预处理训练数据"]},{"cell_type":"code","execution_count":7,"metadata":{"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'process_func' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m      3\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_pandas(train_df)\n\u001b[0;32m----> 4\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m train_ds\u001b[38;5;241m.\u001b[39mmap(\u001b[43mprocess_func\u001b[49m, remove_columns\u001b[38;5;241m=\u001b[39mtrain_ds\u001b[38;5;241m.\u001b[39mcolumn_names)\n","\u001b[0;31mNameError\u001b[0m: name 'process_func' is not defined"]}],"source":["from datasets import Dataset\n","\n","train_ds = Dataset.from_pandas(train_df)\n","train_dataset = train_ds.map(process_func, remove_columns=train_ds.column_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import torch\n","from transformers import AutoTokenizer\n","\n","# # 初始化tokenizer，这里需要您根据实际情况替换为您的模型对应的tokenizer\n","# tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n","\n","def process_func(example):\n","    \"\"\"\n","    将数据集进行预处理\n","    \"\"\"\n","    MAX_LENGTH = 384\n","    input_ids, attention_mask = [], []\n","    \n","    # 构建指令部分\n","    instruction = tokenizer(\n","        f\"<|im_start|>system\\n{example['instruction']}<|im_end|>\\n<|im_start|>user\\n{example['input']}<|im_end|>\\n<|im_start|>assistant\\n\",\n","        add_special_tokens=False,\n","    )\n","    \n","    # 构建响应部分，即输出部分\n","    response = tokenizer(\"\", add_special_tokens=False)  # 这里不需要再次进行tokenizer处理\n","    \n","    # 合并指令和响应的input_ids和attention_mask\n","    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"]\n","    attention_mask = instruction[\"attention_mask\"] + response[\"attention_mask\"]\n","    \n","    # 如果长度超过MAX_LENGTH，进行截断\n","    if len(input_ids) > MAX_LENGTH:\n","        input_ids = input_ids[:MAX_LENGTH]\n","        attention_mask = attention_mask[:MAX_LENGTH]\n","    \n","    # 将labels转换为浮点数类型的张量\n","    labels = []\n","    for key in ['personality_conscientiousness', 'personality_openess', 'personality_extraversion', 'personality_agreeableness', 'personality_stability']:\n","        try:\n","            labels.append(float(example['output'][key]))\n","        except (ValueError, KeyError):\n","            # 如果转换失败或键不存在，可以用0或平均值填充，或者选择跳过这个样本\n","            labels.append(0)  # 或者选择其他合适的默认值\n","    \n","    labels = torch.tensor(labels, dtype=torch.float)\n","    \n","    return {\n","        \"input_ids\": input_ids + [tokenizer.pad_token_id],  # 添加padding\n","        \"attention_mask\": attention_mask + [1],  # 添加padding\n","        \"labels\": labels\n","    }\n","\n","# 示例数据\n","example = {\n","    \"instruction\": \"根据以下评论内容，预测评论者的个性特质。\",\n","    \"input\": \"It breaks my heart to see people living in those conditions...\",\n","    \"output\": {\n","        \"personality_conscientiousness\": \"7\",\n","        \"personality_openess\": \"5.5\",\n","        \"personality_extraversion\": \"1\",\n","        \"personality_agreeableness\": \"6.5\",\n","        \"personality_stability\": \"6\"\n","    }\n","}\n","\n","# 测试process_func函数\n","processed_example = process_func(example)\n","print(processed_example)"]},{"cell_type":"markdown","metadata":{},"source":["## 5. 设置LORA"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from peft import LoraConfig, TaskType, get_peft_model\n","\n","config = LoraConfig(\n","    task_type=TaskType.CAUSAL_LM,\n","    target_modules=[\n","        \"q_proj\",\n","        \"k_proj\",\n","        \"v_proj\",\n","        \"o_proj\",\n","        \"gate_proj\",\n","        \"up_proj\",\n","        \"down_proj\",\n","    ],\n","    inference_mode=False,  # 训练模式\n","    r=8,  # Lora 秩\n","    lora_alpha=32,  # Lora alaph，具体作用参见 Lora 原理\n","    lora_dropout=0.1,  # Dropout 比例\n",")\n","\n","model = get_peft_model(model, config)"]},{"cell_type":"markdown","metadata":{},"source":["## 6. 训练"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["args = TrainingArguments(\n","    output_dir=\"./output/Qwen2-personality\",\n","    per_device_train_batch_size=2,\n","    gradient_accumulation_steps=111,\n","    logging_steps=10,\n","    num_train_epochs=4,\n","    save_steps=100,\n","    learning_rate=1e-4,\n","    save_on_each_node=True,\n","    gradient_checkpointing=True,\n","    report_to=\"none\",\n","   \n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from swanlab.integration.huggingface import SwanLabCallback\n","import swanlab\n","\n","swanlab_callback = SwanLabCallback(\n","    project=\"Qwen2-fintune\",\n","    experiment_name=\"Qwen2-1.5B-Instruct\",\n","    description=\"使用通义千问Qwen2-1.5B-Instruct模型在personality数据集上微调。\",\n","    config={\n","        \"model\": \"qwen/Qwen2-1.5B-Instruct\",\n","        \"dataset\": \"illusion/personality\",\n","    },\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["trainer = Trainer(\n","    model=model,\n","    args=args,\n","    train_dataset=train_dataset,\n","    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n","    callbacks=[swanlab_callback],\n",")\n","\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# ====== 训练结束后的预测 ===== #\n","\n","def predict(messages, model, tokenizer):\n","    device = \"cuda\"\n","    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n","    generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=512)\n","    generated_ids = [\n","        output_ids[len(input_ids) :]\n","        for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n","    ]\n","\n","    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n","    print(response)\n","\n","    return response\n","    \n","\n","test_text_list = []\n","for index, row in test_df.iterrows():\n","    instruction = row[\"instruction\"]\n","    input_value = row[\"input\"]\n","\n","    messages = [\n","        {\"role\": \"system\", \"content\": f\"{instruction}\"},\n","        {\"role\": \"user\", \"content\": f\"{input_value}\"},\n","    ]\n","\n","    response = predict(messages, model, tokenizer)\n","    messages.append({\"role\": \"assistant\", \"content\": f\"{response}\"})\n","    result_text = f\"{messages[0]}\\n\\n{messages[1]}\\n\\n{messages[2]}\"\n","    test_text_list.append(swanlab.Text(result_text, caption=response))\n","\n","swanlab.log({\"Prediction\": test_text_list})\n","swanlab.finish()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["=======\n","import pandas as pd\n","import torch\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer\n","\n","# 假设模型和分词器已经被正确初始化\n","# model = AutoModelForSequenceClassification.from_pretrained('model_name')\n","# tokenizer = AutoTokenizer.from_pretrained('model_name')\n","\n","def predict(input_text, model, tokenizer):\n","    \"\"\"\n","    使用模型预测性格特质评分\n","    \"\"\"\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n","    with torch.no_grad():\n","        outputs = model(input_ids['input_ids'])\n","        logits = outputs.logits\n","        predictions = torch.softmax(logits, dim=-1)\n","    return predictions.cpu().numpy()[0]\n","\n","# 测试数据\n","predictions_list = []\n","for index, row in test_df.iterrows():\n","    input_text = row['comment']\n","    predictions = predict(input_text, model, tokenizer)\n","    \n","    # 将预测结果添加到列表\n","    predictions_list.append({\n","        \"personality_conscientiousness\": predictions[0],\n","        \"personality_openess\": predictions[1],\n","        \"personality_extraversion\": predictions[2],\n","        \"personality_agreeableness\": predictions[3],\n","        \"personality_stability\": predictions[4]\n","    })\n","\n","# 将预测结果转换为 DataFrame\n","predictions_df = pd.DataFrame(predictions_list)\n","\n","# 保存 DataFrame 为 TSV 文件\n","output_file_path = \"/home/wangyanan/transformer-code1/transformers-code/12-qwen2.5-7b/Qwen/LLM-Finetune/dataset/predictions.tsv\"\n","predictions_df.to_csv(output_file_path, sep='\\t', index=False)\n","\n","print(f\"预测结果已保存至：{output_file_path}\")"]},{"cell_type":"code","execution_count":128,"metadata":{"execution":{"iopub.execute_input":"2024-10-12T15:58:04.710864Z","iopub.status.busy":"2024-10-12T15:58:04.710454Z","iopub.status.idle":"2024-10-12T15:58:04.716538Z","shell.execute_reply":"2024-10-12T15:58:04.715509Z","shell.execute_reply.started":"2024-10-12T15:58:04.710828Z"},"trusted":true},"outputs":[],"source":["from swanlab.integration.huggingface import SwanLabCallback\n","import swanlab\n","\n","swanlab_callback = SwanLabCallback(\n","    project=\"Qwen2-fintune\",\n","    experiment_name=\"Qwen2-1.5B-Instruct\",\n","    description=\"使用通义千问Qwen2-1.5B-Instruct模型在personality数据集上微调。\",\n","    config={\n","        \"model\": \"qwen/Qwen2-1.5B-Instruct\",\n","        \"dataset\": \"illusion/personality\",\n","    },\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-12T15:58:15.691845Z","iopub.status.busy":"2024-10-12T15:58:15.691137Z","iopub.status.idle":"2024-10-12T15:59:47.263664Z","shell.execute_reply":"2024-10-12T15:59:47.262664Z","shell.execute_reply.started":"2024-10-12T15:58:15.691804Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import json\n","import os\n","from modelscope import snapshot_download, AutoTokenizer\n","from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n","import torch\n","import torch.nn as nn\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","def dataset_tsv_to_jsonl(origin_path, new_path):\n","    df = pd.read_csv(origin_path, sep='\\t')\n","    messages = []\n","    for index, row in df.iterrows():\n","        message = {\n","            \"instruction\": \"根据以下评论内容，预测评论者的个性特质。\",\n","            \"input\": row['comment'],\n","            \"output\": {\n","                \"personality_conscientiousness\": row['personality_conscientiousness'],\n","                \"personality_openess\": row['personality_openess'],\n","                \"personality_extraversion\": row['personality_extraversion'],\n","                \"personality_agreeableness\": row['personality_agreeableness'],\n","                \"personality_stability\": row['personality_stability']\n","            }\n","        }\n","        messages.append(message)\n","    with open(new_path, \"w\", encoding=\"utf-8\") as file:\n","        for message in messages:\n","            file.write(json.dumps(message, ensure_ascii=False) + \"\\n\")\n","\n","train_tsv_path = \"/home/wangyanan/transformer-code1/transformers-code/13-qwen2.5-7b/dataset/train.tsv\"\n","new_train_path = \"/home/wangyanan/transformer-code1/transformers-code/13-qwen2.5-7b/dataset/new_personality_train.jsonl\"\n","if not os.path.exists(new_train_path):\n","    dataset_tsv_to_jsonl(train_tsv_path, new_train_path)\n","\n","train_df = pd.read_json(new_train_path, lines=True)\n","\n","class PersonalityDataset(Dataset):\n","    def __init__(self, dataframe, tokenizer, max_length):\n","        self.dataframe = dataframe\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.dataframe)\n","\n","    def __getitem__(self, idx):\n","        item = self.dataframe.iloc[idx]\n","        inputs = self.tokenizer(\n","            item['input'],\n","            max_length=self.max_length,\n","            padding='max_length',\n","            truncation=True,\n","            return_attention_mask=True,\n","            return_tensors='pt'\n","        )\n","        labels = []\n","        for key in ['personality_conscientiousness', 'personality_openess', 'personality_extraversion', 'personality_agreeableness', 'personality_stability']:\n","            try:\n","                labels.append(float(item['output'][key]))\n","            except (ValueError, KeyError):\n","                labels.append(0)\n","        labels = torch.tensor(labels, dtype=torch.float)\n","        return {\n","            \"input_ids\": inputs['input_ids'].flatten(),\n","            \"attention_mask\": inputs['attention_mask'].flatten(),\n","            \"labels\": labels\n","        }\n","\n","# 加载预训练模型和tokenizer\n","\n","\n","# 加载分词器和模型\n","tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n","model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)\n","\n","# 现在模型和分词器已经准备好，可以用于进一步的处理或训练。\n","\n","# 替换最后的输出层以适应回归任务\n","model.classifier = nn.Linear(model.classifier.in_features, 5)  # 假设有5个个性特质分数\n","\n","max_length = 384\n","dataset = PersonalityDataset(train_df, tokenizer, max_length)\n","batch_size = 8\n","train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","\n","args = TrainingArguments(\n","    output_dir=\"./output/Qwen2-personality\",\n","    per_device_train_batch_size=batch_size,\n","    gradient_accumulation_steps=4,\n","    logging_steps=10,\n","    num_train_epochs=2,\n","    save_steps=100,\n","    learning_rate=1e-4,\n","    save_on_each_node=True,\n","    gradient_checkpointing=True,\n","    report_to=\"none\",\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=args,\n","    train_dataset=dataset,\n","    data_collator=None,  # 使用默认的数据整理器\n",")\n","\n","trainer.train()\n","swanlab.log({\"Prediction\": test_text_list})\n","swanlab.finish()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import json\n","import os\n","\n","from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n","import torch\n","import torch.nn as nn\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n","model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5861494,"sourceId":9606811,"sourceType":"datasetVersion"},{"isSourceIdPinned":true,"modelId":137712,"modelInstanceId":114446,"sourceId":135304,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3.9.20 64-bit ('qwen': conda)","metadata":{"interpreter":{"hash":"aed5936ed1f22c449b9ef9e957a14c9173742fb3b917be219d501cac6fbc754b"}},"name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.20"}},"nbformat":4,"nbformat_minor":4}
